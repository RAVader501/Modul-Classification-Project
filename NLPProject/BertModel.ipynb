{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c93a1b68",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\vader\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\vader\\anaconda3\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\vader\\anaconda3\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\vader\\anaconda3\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\vader\\anaconda3\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\vader\\anaconda3\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\vader\\anaconda3\\lib\\site-packages (from torch) (2023.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\vader\\anaconda3\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\vader\\anaconda3\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "#Installing packages\n",
    "!pip install torch\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edf80856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titelde</th>\n",
       "      <th>SDGs</th>\n",
       "      <th>lernergebnissedeUNDlerninhaltede</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Webtechnologien</td>\n",
       "      <td>4, 9</td>\n",
       "      <td>Studierende, die dieses Modul erfolgreich absc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Informationssysteme und Datenanalyse</td>\n",
       "      <td>4, 9, 11, 17</td>\n",
       "      <td>Informationssysteme bilden die Basis fuer fast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nachrichtenuebertragung mit Praktikum</td>\n",
       "      <td>4, 9</td>\n",
       "      <td>Das wesentliche Qualifikationsziel dieses Modu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nachrichtenuebertragung</td>\n",
       "      <td>4, 9</td>\n",
       "      <td>Nach dem erfolgreichen Abschluss des Moduls si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Projekt: Brain-Computer Interfacing</td>\n",
       "      <td>3, 4, 9</td>\n",
       "      <td>Die Absolventinnen und Absolventen dieses Modu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>Machine Learning for Computer Security</td>\n",
       "      <td>4, 9</td>\n",
       "      <td>Die Studierenden haben ein umfassendes Verstae...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Machine Learning and Security - Project</td>\n",
       "      <td>4, 9</td>\n",
       "      <td>Nach Abschluss des Projekts verfuegen die Stud...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>Machine Learning and Security - Bachelor Seminar</td>\n",
       "      <td>4, 9</td>\n",
       "      <td>Im Rahmen des Seminars erwerben die Studierend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>Machine Learning and Security - Master Seminar</td>\n",
       "      <td>4, 9</td>\n",
       "      <td>Im Rahmen des Seminars erwerben die Studierend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>Projektorientierte App- und Web-Entwicklung</td>\n",
       "      <td>4, 9</td>\n",
       "      <td>In der Projektwerkstatt sollen die folgenden K...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titelde          SDGs  \\\n",
       "0                                     Webtechnologien          4, 9   \n",
       "1                Informationssysteme und Datenanalyse  4, 9, 11, 17   \n",
       "2               Nachrichtenuebertragung mit Praktikum          4, 9   \n",
       "3                             Nachrichtenuebertragung          4, 9   \n",
       "4                 Projekt: Brain-Computer Interfacing       3, 4, 9   \n",
       "..                                                ...           ...   \n",
       "393            Machine Learning for Computer Security          4, 9   \n",
       "394           Machine Learning and Security - Project          4, 9   \n",
       "395  Machine Learning and Security - Bachelor Seminar          4, 9   \n",
       "396    Machine Learning and Security - Master Seminar          4, 9   \n",
       "397       Projektorientierte App- und Web-Entwicklung          4, 9   \n",
       "\n",
       "                      lernergebnissedeUNDlerninhaltede  \n",
       "0    Studierende, die dieses Modul erfolgreich absc...  \n",
       "1    Informationssysteme bilden die Basis fuer fast...  \n",
       "2    Das wesentliche Qualifikationsziel dieses Modu...  \n",
       "3    Nach dem erfolgreichen Abschluss des Moduls si...  \n",
       "4    Die Absolventinnen und Absolventen dieses Modu...  \n",
       "..                                                 ...  \n",
       "393  Die Studierenden haben ein umfassendes Verstae...  \n",
       "394  Nach Abschluss des Projekts verfuegen die Stud...  \n",
       "395  Im Rahmen des Seminars erwerben die Studierend...  \n",
       "396  Im Rahmen des Seminars erwerben die Studierend...  \n",
       "397  In der Projektwerkstatt sollen die folgenden K...  \n",
       "\n",
       "[398 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing the data\n",
    "module_data = pd.read_csv('German_cleaned_combined2.csv')\n",
    "display(module_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fb83ab3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['4', '9'],\n",
       " ['4', '9', '11', '17'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9', '11', '12', '13', '16'],\n",
       " ['4', '9', '11', '12', '13', '16'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '7', '9', '12', '13'],\n",
       " ['4', '9', '11', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '16'],\n",
       " ['4', '16'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '5'],\n",
       " ['4', '5', '9', '16'],\n",
       " ['4', '9', '11', '17'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '8', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '7', '9', '13'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11', '12'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '7', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['9'],\n",
       " ['9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['9', '11'],\n",
       " ['9', '11'],\n",
       " ['4', '9', '11'],\n",
       " ['9'],\n",
       " ['9', '12'],\n",
       " ['9'],\n",
       " ['7', '9', '13'],\n",
       " ['7', '9', '13'],\n",
       " ['9', '12'],\n",
       " ['7', '9', '12', '13'],\n",
       " ['9'],\n",
       " ['4', '9'],\n",
       " ['7', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['8'],\n",
       " ['4', '9'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['7', '9', '11', '12', '13'],\n",
       " ['9'],\n",
       " ['4', '9'],\n",
       " ['9'],\n",
       " ['9'],\n",
       " ['9'],\n",
       " ['9', '12'],\n",
       " ['9', '12'],\n",
       " ['12'],\n",
       " ['4', '9'],\n",
       " ['9'],\n",
       " ['9'],\n",
       " ['9', '11'],\n",
       " ['3', '9'],\n",
       " ['4', '9'],\n",
       " ['9'],\n",
       " ['4', '9', '17'],\n",
       " ['7', '9', '12'],\n",
       " ['9', '11'],\n",
       " ['9'],\n",
       " ['9', '12'],\n",
       " ['9', '12'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9', '17'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '12'],\n",
       " ['7', '9', '11'],\n",
       " ['9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['7', '9', '11'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['7', '9', '12'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '17'],\n",
       " ['4', '9'],\n",
       " ['7', '9', '11'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['8'],\n",
       " ['8'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['3', '9', '11'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '7', '9', '13'],\n",
       " ['4', '7', '9', '13'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '7', '9', '11', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['9', '16'],\n",
       " ['4', '9', '11', '12'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '7', '9', '11', '13'],\n",
       " ['4', '9', '11'],\n",
       " ['2008', '9', '12'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['9', '12'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['9', '12'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['4', '7', '9', '11'],\n",
       " ['4', '7', '9', '11'],\n",
       " ['4', '7', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['7', '9', '12', '13'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '7', '9', '11'],\n",
       " ['9', '16'],\n",
       " ['4', '9'],\n",
       " ['4', '7', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '8', '9', '16'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['4', '7', '9', '11', '12'],\n",
       " ['3', '4', '7', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['3', '4', '9'],\n",
       " ['4', '9', '11', '12'],\n",
       " ['4', '9', '11', '12'],\n",
       " ['4', '9'],\n",
       " ['7', '9', '11'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['7', '9', '11'],\n",
       " ['7', '9', '11'],\n",
       " ['7', '9', '11'],\n",
       " ['7', '9', '11'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['9', '11'],\n",
       " ['7', '9', '11'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['9', '11'],\n",
       " ['7', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['7', '9', '13'],\n",
       " ['9'],\n",
       " ['9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '7', '9', '11', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '7', '9', '11', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11', '17'],\n",
       " ['4', '9'],\n",
       " ['4', '8', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '7', '9', '11'],\n",
       " ['4', '7', '9', '12'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['4', '9'],\n",
       " ['4', '16'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '8', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '8', '9', '17'],\n",
       " ['4', '16'],\n",
       " ['4', '8', '9', '16'],\n",
       " ['4', '8', '9', '16'],\n",
       " ['4', '8', '9', '17'],\n",
       " ['4', '16'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9', '16', '17'],\n",
       " ['3', '4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['9', '11'],\n",
       " ['4', '9', '11'],\n",
       " ['9', '11'],\n",
       " ['8', '9', '11', '12'],\n",
       " ['4', '7', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['4', '9'],\n",
       " ['4'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['7', '9'],\n",
       " ['7', '9', '11'],\n",
       " ['7', '9', '11'],\n",
       " ['7', '9', '11', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9'],\n",
       " ['4', '7', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '17'],\n",
       " ['4', '3', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '7', '9'],\n",
       " ['4', '5'],\n",
       " ['4', '5'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9', '12'],\n",
       " ['4', '9', '17'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '11'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '11'],\n",
       " ['4', '8', '9'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['3', '4', '9'],\n",
       " ['7', '9', '11', '13'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9', '16'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9'],\n",
       " ['4', '9']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Preprocessing the dataset\n",
    "descriptions = module_data['lernergebnissedeUNDlerninhaltede'].tolist()\n",
    "\n",
    "module_data['SDGs'] = module_data['SDGs'].apply(lambda x: [label.strip() for label in x.split(',')])\n",
    "labels = module_data['SDGs'].tolist()\n",
    "\n",
    "\n",
    "display(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "982c70ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 1]\n",
      " [1 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " ...\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "# Using MultiLabelBinarizer for multi-class classification\n",
    "mlb = MultiLabelBinarizer()\n",
    "labels = mlb.fit_transform(labels)\n",
    "\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50f06e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the input texts\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-german-cased')\n",
    "tokenized_descriptions = tokenizer(descriptions, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a254856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a custom dataset class\n",
    "class SDGDataset(Dataset):\n",
    "    def __init__(self, tokenized_texts, labels):\n",
    "        self.tokenized_texts = tokenized_texts\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.tokenized_texts['input_ids'][idx],\n",
    "            'attention_mask': self.tokenized_texts['attention_mask'][idx],\n",
    "            'labels': torch.tensor(self.labels[idx])\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9a2a4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creatign an instances of the custom dataset class\n",
    "dataset = SDGDataset(tokenized_descriptions, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "482bc73c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and validation sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "669ae316",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating DataLoader instances\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd7f3959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Loading the pre-trained BERT model for German\n",
    "num_sdgs = len(mlb.classes_)\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-german-cased', num_labels=num_sdgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0fee22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vader\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Defining training parameters\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "69dd5148",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Validation Loss: 0.19381624991074203\n",
      "Epoch 2/10, Validation Loss: 0.19674797793850302\n",
      "Epoch 3/10, Validation Loss: 0.19684996847063302\n",
      "Epoch 4/10, Validation Loss: 0.20145150311291218\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m}\n\u001b[0;32m     11\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)  \u001b[38;5;66;03m# Move labels to the same device as inputs\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m     13\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mlogits, labels)\n\u001b[0;32m     14\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1013\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1015\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[0;32m   1016\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[0;32m   1017\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1021\u001b[0m )\n\u001b[1;32m-> 1022\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[0;32m   1023\u001b[0m     embedding_output,\n\u001b[0;32m   1024\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[0;32m   1025\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[0;32m   1026\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[0;32m   1027\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[0;32m   1028\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[0;32m   1029\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[0;32m   1030\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m   1031\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[0;32m   1032\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1033\u001b[0m )\n\u001b[0;32m   1034\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1035\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:612\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    603\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mcheckpoint\u001b[38;5;241m.\u001b[39mcheckpoint(\n\u001b[0;32m    604\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    605\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    609\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    610\u001b[0m     )\n\u001b[0;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[0;32m    613\u001b[0m         hidden_states,\n\u001b[0;32m    614\u001b[0m         attention_mask,\n\u001b[0;32m    615\u001b[0m         layer_head_mask,\n\u001b[0;32m    616\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    617\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    618\u001b[0m         past_key_value,\n\u001b[0;32m    619\u001b[0m         output_attentions,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    485\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    487\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    494\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[0;32m    496\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[0;32m    498\u001b[0m         hidden_states,\n\u001b[0;32m    499\u001b[0m         attention_mask,\n\u001b[0;32m    500\u001b[0m         head_mask,\n\u001b[0;32m    501\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[0;32m    502\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[0;32m    503\u001b[0m     )\n\u001b[0;32m    504\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    418\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    419\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    425\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    426\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m--> 427\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[0;32m    428\u001b[0m         hidden_states,\n\u001b[0;32m    429\u001b[0m         attention_mask,\n\u001b[0;32m    430\u001b[0m         head_mask,\n\u001b[0;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    433\u001b[0m         past_key_value,\n\u001b[0;32m    434\u001b[0m         output_attentions,\n\u001b[0;32m    435\u001b[0m     )\n\u001b[0;32m    436\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[0;32m    437\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:355\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    352\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m attention_scores \u001b[38;5;241m+\u001b[39m attention_mask\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[1;32m--> 355\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(attention_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[0;32m    359\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_probs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:1856\u001b[0m, in \u001b[0;36msoftmax\u001b[1;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[0;32m   1854\u001b[0m     dim \u001b[38;5;241m=\u001b[39m _get_softmax_dim(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim(), _stacklevel)\n\u001b[0;32m   1855\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1856\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1858\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msoftmax(dim, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        inputs = {k: v.to(model.device) for k, v in batch.items() if k != 'labels'}\n",
    "        labels = batch['labels'].float().to(model.device)  # Move labels to the same device as inputs\n",
    "        outputs = model(**inputs)\n",
    "        loss = criterion(outputs.logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    for batch in val_dataloader:\n",
    "        with torch.no_grad():\n",
    "            inputs = {k: v.to(model.device) for k, v in batch.items() if k != 'labels'}\n",
    "            labels = batch['labels'].float().to(model.device)  # Move labels to the same device as inputs\n",
    "            outputs = model(**inputs)\n",
    "            val_loss += criterion(outputs.logits, labels).item()\n",
    "\n",
    "    val_loss /= len(val_dataloader)\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs}, Validation Loss: {val_loss}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f169d96f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Die Studierenden sind befaehigt, digitale Systeme in einer Hardwarebeschreibungssprache (VHDL) zu beschreiben und deren Verhalten mit Hilfe der eingesetzten Entwicklungswerkzeuge zu simulieren. Sie sind in der Lage, synchrone und asynchrone hierarchische Designs mit Verhaltens- und Strukturbeschreibungen zu implementieren. Sie beherrschen sequentielle und nebenlaeufige Sprachkonstrukte. Sie kennen den Aufbau und die Funktionsweise einer Testbench und sind in der Lage, diese eigenstaendig zu entwickeln. Sie haben ausserdem Erfahrung mit dem systematischen Debugging von komplexen Schaltungen gesammelt. - Beschreibung digitaler Systeme mit einer Hardwarebeschreibungssprache (VHDL) - Wesentliche Sprachkonstrukte in VHDL (hierarchisches Design mit Verhaltens- und Strukturbeschreibungen, synchrone und asynchrone sowie sequentielle und nebenlaeufige Beschreibungen) - Beschreibung einzelner Komponenten eines RISC-Prozessors - Simulation digitaler Schaltungen (Umgang mit Entwicklungswerkzeugen, Entwicklung von Testbenches, systematisches Debugging)\n",
      "Predicted labels: ['8', '12']\n"
     ]
    }
   ],
   "source": [
    "#Testing the model\n",
    "from torch.nn.functional import sigmoid\n",
    "\n",
    "text = \"Das Modul behandelt weiterfuehrende Themen der Logik in der Informatik. Absolventen und Absolventinnen der Kurses haben ein gutes Verstaendnis fuer Anwendungen der Logik in der Informatik. Sie koennen souveraen mit logischen Systemen arbeiten, deren Komplexitaet und Ausdrucksstaerke analysieren.  Sie koennen die Anwendungsmoeglichkeiten der Logik in der Informatik gut einschaetzen und kennen einige wichtige Anwendungen der Logik z.B. in der Verifikation, der Algorithmik und Komplexitaetstheorie sowie zur Beschreibung formaler Sprachen.  Logik, Sprachen und Automaten * Logische Beschreibung regulaerer Sprachen  * Zusammenhang zwischen Logiken und Automaten Verifikation * Verifikationslogiken LTL, CTL und den mu-Kalkuel * Techniken zur automatischen Verifikation logischer Spezifikationen in Transitionssystemen Logik im Kontext relationaler Datenbanken * conjunctive queries * relationaler Kalkuel * effizientes Auswerten von Datenbankanfragen Aussdrucksstaerke logischer Systeme * Definierbarkeit in der Praedikatenlogik * Ehrenfeucht-Fraisse Spiele und  Lokalitaetssaetze Logik und Kopmplexitaet * Auswerten logischer Formeln in Strukturen, besonders Graphen. * Logische Beschreibungen von Komplexitaetsklassen Logik in der kuenstlichen Intelligenz * Constraint Satisfaction Probleme in der kuenstlichen Intelligenz\"\n",
    "\n",
    "#Tokenizng and passing the text to the model\n",
    "inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors='pt').to(model.device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Appling the sigmoid function to the output logits to get the probabilities\n",
    "probs = sigmoid(outputs.logits)\n",
    "threshold = 0.15\n",
    "labels = (probs > threshold).int()\n",
    "\n",
    "#For the testing and comparising sake, leaving labels names as numbers\n",
    "label_names = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\"] \n",
    "label_names = [name for label, name in zip(labels[0], label_names) if label == 1]\n",
    "\n",
    "print(f'Text: {text}')\n",
    "print(f'Predicted labels: {label_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7204158",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304a79a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
